<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Kinematics-Aware Diffusion Policy with Consistent 3D Observation and Action Space for Whole-Arm Robotic Manipulation">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Kinematics-Aware Diffusion Policy</title>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/misc.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Kinematics-Aware Diffusion Policy<br />
          with Consistent 3D Observation and Action Space<br />
          for Whole-Arm Robotic Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Author(s)
            </span>
          </div>
          
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <img src='figures/top_fig.png' width="60%">
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        The proposed <b>KADP</b> uses a set of 3D nodes on the arm body as both robot state and action representation for whole-arm manipulation, which is consistent with the 3D point cloud observation space and task space. Compared with using end-effector poses or joint angles, our method achieves higher spatial generalizability and sample efficiency while ensuring kinematic feasibility.
        </h2>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">

          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
            Whole-body control of robotic manipulators with awareness of full-arm kinematics is crucial for many manipulation scenarios involving body collision avoidance or body-object interactions, which makes it insufficient to consider only the end-effector poses in policy learning. The typical approach for whole-arm manipulation is to learn actions in the robot's joint space. However, the unalignment between the joint space and actual task space (i.e., 3D space) increases the complexity of policy learning, as generalization in task space requires the policy to intrinsically understand the non-linear arm kinematics, which is difficult to learn from limited demonstrations. To address this issue, this letter proposes a kinematics-aware imitation learning framework with consistent task, observation, and action spaces, all represented in the same 3D space. Specifically, we represent both robot states and actions using a set of 3D points on the arm body, naturally aligned with the 3D point cloud observations. This spatially consistent representation improves the policy's sample efficiency and spatial generalizability while enabling full-body control. Built upon the diffusion policy, we further incorporate kinematics priors into the diffusion processes to guarantee the kinematic feasibility of output actions. The joint angle commands are finally calculated through an optimization-based whole-body inverse kinematics solver for execution. Simulation and real-world experimental results demonstrate higher success rates and stronger spatial generalizability of our approach compared to existing methods in body-aware manipulation learning.
            </p>
          </div>
          <br>
          <h2 class="title is-3">Video</h2>
          <video id="teaser" muted height="100%" width="100%" controls="controls">
              <source src="videos/ral_submission.mp4" type="video/mp4">
            </video>

          <!-- <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/DQHxb4saxQo" frameborder="0" allow="autoplay; encrypted-media"
              allowfullscreen></iframe> -->
          <!-- <video id="teaser" autoplay muted loop height="100%" width="100%">
            <source src="media/videos/main.mp4"
                    type="video/mp4">
          </video> -->
   		  </div>
      </div>
    </div>
  </div>
</section>

<section class="hero">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="container is-max-widescreen">

        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <p>Overview of <b>Kinematics-Aware Diffusion Policy (KADP)</b>. Taking the encoded 3D visual representations, the 3D robot nodes and time embeddings as input, diffusion model predicts the denoised 3D node trajectory iteratively. For execution, the joint angle commands are computed through an optimization-based whole-body inverse kinematics solver.
          </p>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column is-fullwidth">
            <!-- <video id="teaser" autoplay muted loop height="100%" width="100%">
            <source src="media/videos/method.mp4"
                    type="video/mp4">
          </video> -->
          <div class="column is-fullwidth">
            <img src="figures/framework.png" alt="" width="90%">
          </div>
          </div>
        </div>

        <h2 class="title is-3">Task1: Pick up Cube</h2>
        
        <div class="columns">
          <div class="column has-text-centered">
            <video id="dist1" controls muted autoplay loop width="80%">
              <source src="videos/pick_up_cube_pose1.mp4" type="video/mp4">
            </video>
            <h2 class="title is-5">DP3-EE</h2>
          </div>

          <div class="column has-text-centered">
            <video id="dist1" controls muted autoplay loop width="80%">
              <source src="videos/pick_up_cube_joint1.mp4" type="video/mp4">
            </video>
            <h2 class="title is-5">DP3-Joint</h2>
          </div>

          <div class="column has-text-centered">
            <video id="dist1" controls muted autoplay loop width="80%">
              <source src="videos/pick_up_cube_node1.mp4" type="video/mp4">
            </video>
            <h2 class="title is-5">KADP (Ours)</h2>
          </div>
        </div>
        
        <!-- <h2 class="title is-3">Material-Adaptive Dynamics</h2>
        <div class="content has-text-justified">
          <p> Our method can capture the dynamics of various object types such as rigid boxes, ropes, granular objects, and cloths. 
            The red dashed boxes highlight the improved dynamics prediction accuracy of our method compared to the no adaptation baseline. 
          </p>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column is-fullwidth">
            <img src="media/figures/dynamics-1-1-left.jpg" alt="">
            <p style="text-align:center">
              The baseline assumes that the center of pressure is at the geometric center of the box, 
              and predicts the wrong rotation direction.
            </p>
          </div>
          <div class="column is-fullwidth">
            <img src="media/figures/dynamics-1-1-right.jpg" alt="">
            <p style="text-align:center">
              Our method identifies that the yarn has low stiffness, and predicts more bending effects.
            </p>
          </div>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column is-fullwidth">
            <img src="media/figures/dynamics-1-2-left.jpg" alt="">
            <p style="text-align:center">
              The baseline assumes that the center of pressure is at the geometric center of the box, 
              and predicts the wrong rotation direction.
            </p>
          </div>
          <div class="column is-fullwidth">
            <img src="media/figures/dynamics-1-2-right.jpg" alt="">
            <p style="text-align:center">
              Our method identifies that the polymer rope has high stiffness, and predictes less bending effects.
            </p>
          </div>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column is-fullwidth">
            <img src="media/figures/dynamics-1-3-left.jpg" alt="">
            <p style="text-align:center">
              Our method identifies that the coffee beans have small granularity, and predicts more stacking effects.
            </p>
          </div>
          <div class="column is-fullwidth">
            <img src="media/figures/dynamics-1-3-right.jpg" alt="">
            <p style="text-align:center">
              The baseline predicts insufficient stretching and shearing effects for the low-stiffness modal fabric cloth.
            </p>
          </div>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column is-fullwidth">
            <img src="media/figures/dynamics-1-4-left.jpg" alt="">
            <p style="text-align:center">
              Our method identifies that the chocolates have large granularity, and predicts less stacking effects.
            </p>
          </div>
          <div class="column is-fullwidth">
            <img src="media/figures/dynamics-1-4-right.jpg" alt="">
            <p style="text-align:center">
              The baseline predicts too much stretching effects for the high-stiffness cotton cloth.
            </p>
          </div>
        </div>


        <h2 class="title is-3">Adaptation Results</h2>
        <div class="content has-text-justified">
          <p> For unseen instances at test time, we perform online inverse optimization to refine their physical property variables based on the physical behaviors observed during interactions.
            Our estimation rankings align with human impressions of these objects.
          </p>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column is-fullwidth">
            <img src="media/figures/estimation.jpg" alt="">
          </div>
        </div>

        <h2 class="title is-3">Planning</h2>
        <div class="content has-text-justified">
          <p> Finally, our method enhances model-based planning for manipulation of different materials. 
            The estimated physical property variable allows us to find more efficient actions, resulting in less steps to successfully achieve the target configuration and with lower final errors.
          </p>
        </div>
        
        <div class="columns">
          <div class="column has-text-centered">
            <h2 class="title is-5">Box pushing - w/o Adaptation</h2>
            <video id="dist1" controls muted autoplay loop width="99%">
              <source src="media/videos/planning-box-1-baseline.mp4" type="video/mp4">
            </video>
          </div>

          <div class="column has-text-centered">
            <h2 class="title is-5">Box pushing - Ours</h2>
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/planning-box-1-ours.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="columns">
          <div class="column has-text-centered">
              <h2 class="title is-5">Rope straightening - w/o Adaptation</h2>
            <video id="dist1" controls muted autoplay loop width="99%">
              <source src="media/videos/planning-rope-1-baseline.mp4" type="video/mp4">
            </video>
          </div>

          <div class="column has-text-centered">
              <h2 class="title is-5">Rope straightening - Ours</h2>
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/planning-rope-1-ours.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="columns">
          <div class="column has-text-centered">
            <h2 class="title is-5">Granular gathering - w/o Adaptation</h2>
            <video id="dist1" controls muted autoplay loop width="99%">
              <source src="media/videos/planning-granular-1-baseline.mp4" type="video/mp4">
            </video>
          </div>

          <div class="column has-text-centered">
            <h2 class="title is-5">Granular gathering - Ours</h2>
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/planning-granular-1-ours.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="columns">
          <div class="column has-text-centered">
            <h2 class="title is-5">Cloth relocating - w/o Adaptation</h2>
            <video id="dist1" controls muted autoplay loop width="99%">
              <source src="media/videos/planning-cloth-1-baseline.mp4" type="video/mp4">
            </video>
          </div>

          <div class="column has-text-centered">
            <h2 class="title is-5">Cloth relocating - Ours</h2>
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/planning-cloth-1-ours.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <br>
        <h2 class="title is-3">Example Simulation Data</h2>
        <div class="columns">
          <div class="column has-text-centered">
            <h2 class="title is-5">Granular - Low granularity</h2>
            <video id="dist1" controls muted autoplay loop width="99%">
              <source src="media/videos/sim_granular_size_0.mp4" type="video/mp4">
            </video>
          </div>

          <div class="column has-text-centered">
            <h2 class="title is-5">Granular - High granularity</h2>
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/sim_granular_size_1.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="columns">
          <div class="column has-text-centered">
            <h2 class="title is-5">Rope - Low stiffness</h2>
            <video id="dist1" controls muted autoplay loop width="99%">
              <source src="media/videos/sim_rope_stiff_0.mp4" type="video/mp4">
            </video>
          </div>

          <div class="column has-text-centered">
            <h2 class="title is-5">Rope - High stiffness</h2>
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/sim_rope_stiff_1.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="columns">
          <div class="column has-text-centered">
            <h2 class="title is-5">Cloth - Low stiffness</h2>
            <video id="dist1" controls muted autoplay loop width="99%">
              <source src="media/videos/sim_cloth_stiff_0.mp4" type="video/mp4">
            </video>
          </div>

          <div class="column has-text-centered">
            <h2 class="title is-5">Cloth - High stiffness</h2>
            <video id="dist2" controls muted autoplay loop width="99%">
              <source src="media/videos/sim_cloth_stiff_1.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <div class="column has-text-centered">
      <h2 class="title is-3">BibTeX</h2>
    </div>
    <pre><code>@inproceedings{zhang2024adaptigraph,
      title={AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic Manipulation},
      author={Zhang, Kaifeng and Li, Baoyu and Hauser, Kris and Li, Yunzhu},
      booktitle={Proceedings of Robotics: Science and Systems (RSS)},
      year={2024}
    }</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://robopil.github.io/adaptigraph/">AdaptiGraph</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
